{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /opt/anaconda3/lib/python3.9/site-packages (3.5.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /opt/anaconda3/lib/python3.9/site-packages (from pyspark) (0.10.9.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor, LinearRegression, DecisionTreeRegressor, GBTRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import col, dayofmonth, month, year,  to_date, to_timestamp, weekofyear, dayofweek\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "import os\n",
    "import logging\n",
    "\n",
    "logging.getLogger(\"org.apache.spark.scheduler.DAGScheduler\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set JAVA_HOME environment variable explicitly\n",
    "os.environ['JAVA_HOME'] = '/Library/Java/JavaVirtualMachines/jdk-22.jdk/Contents/Home'\n",
    "\n",
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/20 11:49:46 WARN Utils: Your hostname, Geraldines-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 10.0.0.209 instead (on interface en0)\n",
      "24/06/20 11:49:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/20 11:49:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark session\n",
    "my_spark = SparkSession.builder.appName(\"SalesForecast\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+----+-----+----+---+---------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|Year|Month|Week|Day|DayOfWeek|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+----+-----+----+---+---------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|     17850|United Kingdom|2010|   12|  48|  1|        4|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|     17850|United Kingdom|2010|   12|  48|  1|        4|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|     17850|United Kingdom|2010|   12|  48|  1|        4|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|2010-12-01 08:26:00|     3.39|     17850|United Kingdom|2010|   12|  48|  1|        4|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|2010-12-01 08:26:00|     3.39|     17850|United Kingdom|2010|   12|  48|  1|        4|\n",
      "|   536365|    22752|SET 7 BABUSHKA NE...|       2|2010-12-01 08:26:00|     7.65|     17850|United Kingdom|2010|   12|  48|  1|        4|\n",
      "|   536365|    21730|GLASS STAR FROSTE...|       6|2010-12-01 08:26:00|     4.25|     17850|United Kingdom|2010|   12|  48|  1|        4|\n",
      "|   536366|    22633|HAND WARMER UNION...|       6|2010-12-01 08:28:00|     1.85|     17850|United Kingdom|2010|   12|  48|  1|        4|\n",
      "|   536366|    22632|HAND WARMER RED P...|       6|2010-12-01 08:28:00|     1.85|     17850|United Kingdom|2010|   12|  48|  1|        4|\n",
      "|   536367|    84879|ASSORTED COLOUR B...|      32|2010-12-01 08:34:00|     1.69|     13047|United Kingdom|2010|   12|  48|  1|        4|\n",
      "|   536367|    22745|POPPY'S PLAYHOUSE...|       6|2010-12-01 08:34:00|      2.1|     13047|United Kingdom|2010|   12|  48|  1|        4|\n",
      "|   536367|    22748|POPPY'S PLAYHOUSE...|       6|2010-12-01 08:34:00|      2.1|     13047|United Kingdom|2010|   12|  48|  1|        4|\n",
      "|   536367|    22749|FELTCRAFT PRINCES...|       8|2010-12-01 08:34:00|     3.75|     13047|United Kingdom|2010|   12|  48|  1|        4|\n",
      "|   536367|    22310|IVORY KNITTED MUG...|       6|2010-12-01 08:34:00|     1.65|     13047|United Kingdom|2010|   12|  48|  1|        4|\n",
      "|   536367|    84969|BOX OF 6 ASSORTED...|       6|2010-12-01 08:34:00|     4.25|     13047|United Kingdom|2010|   12|  48|  1|        4|\n",
      "|   536367|    22623|BOX OF VINTAGE JI...|       3|2010-12-01 08:34:00|     4.95|     13047|United Kingdom|2010|   12|  48|  1|        4|\n",
      "|   536367|    22622|BOX OF VINTAGE AL...|       2|2010-12-01 08:34:00|     9.95|     13047|United Kingdom|2010|   12|  48|  1|        4|\n",
      "|   536367|    21754|HOME BUILDING BLO...|       3|2010-12-01 08:34:00|     5.95|     13047|United Kingdom|2010|   12|  48|  1|        4|\n",
      "|   536367|    21755|LOVE BUILDING BLO...|       3|2010-12-01 08:34:00|     5.95|     13047|United Kingdom|2010|   12|  48|  1|        4|\n",
      "|   536367|    21777|RECIPE BOX WITH M...|       4|2010-12-01 08:34:00|     7.95|     13047|United Kingdom|2010|   12|  48|  1|        4|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+----+-----+----+---+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file into a DataFrame\n",
    "sales_data = my_spark.read.csv(\"online_retail.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Convert InvoiceDate to a TimestampType\n",
    "sales_data = sales_data.withColumn(\"InvoiceDate\", to_timestamp(sales_data[\"InvoiceDate\"], \"M/d/yyyy H:m\"))\n",
    "\n",
    "# Add the additional date features\n",
    "sales_data = sales_data.withColumn(\"Year\", year(\"InvoiceDate\")) \\\n",
    "       .withColumn(\"Month\", month(\"InvoiceDate\")) \\\n",
    "       .withColumn(\"Week\", weekofyear(\"InvoiceDate\")) \\\n",
    "       .withColumn(\"Day\", dayofmonth(\"InvoiceDate\")) \\\n",
    "       .withColumn(\"DayOfWeek\", dayofweek(\"InvoiceDate\"))\n",
    "\n",
    "sales_data.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=======>                                                   (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+-------+----+-----+----+---+---------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|Country|Year|Month|Week|Day|DayOfWeek|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+-------+----+-----+----+---+---------+\n",
      "|   581587|    22555|PLASTERS IN TIN S...|      12|2011-12-09 12:50:00|     1.65|     12680| France|2011|   12|  49|  9|        6|\n",
      "|   581587|    22367|CHILDRENS APRON S...|       8|2011-12-09 12:50:00|     1.95|     12680| France|2011|   12|  49|  9|        6|\n",
      "|   581587|    22728|ALARM CLOCK BAKEL...|       4|2011-12-09 12:50:00|     3.75|     12680| France|2011|   12|  49|  9|        6|\n",
      "|   581587|    22631|CIRCUS PARADE LUN...|      12|2011-12-09 12:50:00|     1.95|     12680| France|2011|   12|  49|  9|        6|\n",
      "|   581587|    22727|ALARM CLOCK BAKEL...|       4|2011-12-09 12:50:00|     3.75|     12680| France|2011|   12|  49|  9|        6|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+-------+----+-----+----+---+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sales_data.orderBy(\"InvoiceDate\", ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert InvoiceDate to datetime \n",
    "sales_data = sales_data.withColumn(\"InvoiceDate\", to_date(\n",
    "    to_timestamp(col(\"InvoiceDate\"), \"d/M/yyyy H:mm\")))\n",
    "\n",
    "# Aggregate data into daily intervals\n",
    "daily_sales_data = sales_data.groupBy(\"Country\", \"StockCode\", \"InvoiceDate\", \"Year\", \"Month\", \"Day\", \"Week\", \"DayOfWeek\").agg({\"Quantity\": \"sum\",                                                                                                           \"UnitPrice\": \"avg\"})\n",
    "# Rename the target column\n",
    "daily_sales_data = daily_sales_data.withColumnRenamed(\n",
    "    \"sum(Quantity)\", \"Quantity\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.4' currently installed).\n",
      "  from pandas.core import (\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# Split the data into two sets based on the spliting date, \"2011-09-25\". All data up to and including this date should be in the training set, while data after this date should be in the testing set. Return a pandas Dataframe, pd_daily_train_data, containing, at least, the columns [\"Country\", \"StockCode\", \"InvoiceDate\", \"Quantity\"].\n",
    "\n",
    "split_date_train_test = \"2011-09-25\"\n",
    "\n",
    "# Creating the train and test datasets\n",
    "train_data = daily_sales_data.filter(\n",
    "    col(\"InvoiceDate\") <= split_date_train_test)\n",
    "test_data = daily_sales_data.filter(col(\"InvoiceDate\") > split_date_train_test)\n",
    "\n",
    "pd_daily_train_data = train_data.toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Regression Model Building "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/20 11:50:00 WARN DAGScheduler: Broadcasting large task binary with size 1101.6 KiB\n",
      "24/06/20 11:50:01 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "24/06/20 11:50:01 WARN DAGScheduler: Broadcasting large task binary with size 1753.5 KiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# Creating indexer for categorical columns\n",
    "country_indexer = StringIndexer(\n",
    "    inputCol=\"Country\", outputCol=\"CountryIndex\").setHandleInvalid(\"keep\")\n",
    "stock_code_indexer = StringIndexer(\n",
    "    inputCol=\"StockCode\", outputCol=\"StockCodeIndex\").setHandleInvalid(\"keep\")\n",
    "\n",
    "# Selectiong features columns\n",
    "feature_cols = [\"CountryIndex\", \"StockCodeIndex\", \"Month\", \"Year\",\n",
    "                \"DayOfWeek\", \"Day\", \"Week\"]\n",
    "\n",
    "# Using vector assembler to combine features\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "# Initializing a Random Forest model\n",
    "rf = RandomForestRegressor(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"Quantity\",\n",
    "    maxBins=4000\n",
    ")\n",
    "\n",
    "# Create a pipeline for staging the processes\n",
    "pipeline = Pipeline(stages=[country_indexer, stock_code_indexer, assembler, rf])\n",
    "\n",
    "# Training the model\n",
    "model = pipeline.fit(train_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# Getting test predictions\n",
    "test_predictions = model.transform(test_data)\n",
    "test_predictions = test_predictions.withColumn(\n",
    "    \"prediction\", col(\"prediction\").cast(\"double\"))\n",
    "\n",
    "# Provide the Mean Absolute Error (MAE) for your forecast? Return a double/floar \"mae\"\n",
    "\n",
    "# Initializing the evaluator\n",
    "mae_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"Quantity\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "\n",
    "# Obtaining MAE\n",
    "mae = mae_evaluator.evaluate(test_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE) is 30.468210127710204\n"
     ]
    }
   ],
   "source": [
    "print(f'Mean Absolute Error (MAE) is {mae}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Example of Model Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Quantity Sold week 42 of 2011: 238659 Units\n",
      "Actual Quantity Sold week 42 of 2011: 150811 Units\n",
      "Comparison - Predicted vs Actual:\n",
      "Predicted: 238659 Units\n",
      "Actual: 150811 Units\n",
      "Difference: 87848 Units\n"
     ]
    }
   ],
   "source": [
    "# Lets say we want to know how many will be sold week 42 of 2011? \n",
    "\n",
    "weekly_test_predictions = test_predictions.groupBy(\"Year\", \"Week\").agg({\"prediction\": \"sum\"})\n",
    "\n",
    "promotion_week = weekly_test_predictions.filter(col('Week')==42)\n",
    "\n",
    "# Get the predicted quantity for week 42 of 2011\n",
    "predicted_quantity = int(promotion_week.select(\"sum(prediction)\").collect()[0][0])\n",
    "print(f'Predicted Quantity Sold week 42 of 2011: {predicted_quantity} Units')\n",
    "\n",
    "# Filter actual data for week 42 of 2011\n",
    "actual_weekly_sales = test_data.groupBy(\"Year\", \"Week\").agg({\"Quantity\": \"sum\"})\n",
    "actual_quantity = actual_weekly_sales.filter(col('Week') == 42).select(\"sum(Quantity)\").collect()[0][0]\n",
    "\n",
    "print(f'Actual Quantity Sold week 42 of 2011: {actual_quantity} Units')\n",
    "\n",
    "# Compare predicted vs actual\n",
    "if actual_quantity is not None:\n",
    "    print(f'Comparison - Predicted vs Actual:')\n",
    "    print(f'Predicted: {predicted_quantity} Units')\n",
    "    print(f'Actual: {actual_quantity} Units')\n",
    "    difference = predicted_quantity - actual_quantity\n",
    "    print(f'Difference: {difference} Units')\n",
    "else:\n",
    "    print('No actual data found for week 42 of 2011 in the test dataset.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. We can see using one model leads to a terrible forecaster so lets run other models and pick a better "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger(\"org.apache.spark.scheduler.DAGScheduler\").setLevel(logging.ERROR)\n",
    "# Initialize regression models\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"Quantity\", maxBins=4000)\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"Quantity\")\n",
    "dt = DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"Quantity\", maxBins=4000)  # Increase maxBins parameter\n",
    "gbt = GBTRegressor(featuresCol=\"features\", labelCol=\"Quantity\", maxBins=4000)\n",
    "\n",
    "# Create a list of models to iterate over\n",
    "models = [rf, lr, dt, gbt]\n",
    "\n",
    "# Initialize an empty dictionary to store MAEs\n",
    "mae_dict = {}\n",
    "\n",
    "# Iterate over models\n",
    "for model in models:\n",
    "    # Create a pipeline for each model\n",
    "    pipeline = Pipeline(stages=[country_indexer, stock_code_indexer, assembler, model])\n",
    "    \n",
    "    # Train the model\n",
    "    trained_model = pipeline.fit(train_data)\n",
    "    \n",
    "    # Make predictions on test data\n",
    "    test_predictions = trained_model.transform(test_data)\n",
    "    \n",
    "    # Evaluate MAE\n",
    "    evaluator = RegressionEvaluator(labelCol=\"Quantity\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "    mae = evaluator.evaluate(test_predictions)\n",
    "    \n",
    "    # Store MAE in dictionary\n",
    "    model_name = model.__class__.__name__\n",
    "    mae_dict[model_name] = mae\n",
    "    \n",
    "    # Print MAE for each model\n",
    "    print(f\"{model_name} MAE: {mae}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MAE Comparison:\n",
      "RandomForestRegressor: 30.468210127710204\n",
      "LinearRegression: 20.78916333892425\n",
      "DecisionTreeRegressor: 22.999555896800754\n",
      "GBTRegressor: 49.040119594620734\n"
     ]
    }
   ],
   "source": [
    "# Print the MAE dictionary for comparison\n",
    "print(\"\\nMAE Comparison:\")\n",
    "for model_name, mae in mae_dict.items():\n",
    "    print(f\"{model_name}: {mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression is the best regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Using best regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for LinearRegression: 20.09193735248746\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize a new LinearRegression model\n",
    "best_regressor = LinearRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"Quantity\",\n",
    "    maxIter=10,\n",
    "    regParam=0.3,\n",
    "    elasticNetParam=0.8\n",
    ")\n",
    "\n",
    "# Create a new pipeline with the best regressor\n",
    "pipeline = Pipeline(stages=[country_indexer, stock_code_indexer, assembler, best_regressor])\n",
    "\n",
    "# Train the model\n",
    "model = pipeline.fit(train_data)\n",
    "\n",
    "# Get predictions on test data\n",
    "test_predictions = model.transform(test_data)\n",
    "\n",
    "# Evaluate MAE\n",
    "mae_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"Quantity\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"mae\"\n",
    ")\n",
    "\n",
    "mae = mae_evaluator.evaluate(test_predictions)\n",
    "print(f\"MAE for LinearRegression: {mae}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Quantity Sold week 42 of 2011: 118329 Units\n",
      "Actual Quantity Sold week 42 of 2011: 150811 Units\n",
      "Comparison - Predicted vs Actual:\n",
      "Predicted: 118329 Units\n",
      "Actual: 150811 Units\n",
      "Difference: -32482 Units\n"
     ]
    }
   ],
   "source": [
    "# Filter predictions for week 42 of 2011\n",
    "predicted_weekly_sales = test_predictions.groupBy(\"Year\", \"Week\").agg({\"prediction\": \"sum\"})\n",
    "predicted_quantity = int(predicted_weekly_sales.filter(col('Week')==42).select(\"sum(prediction)\").collect()[0][0])\n",
    "print(f'Predicted Quantity Sold week 42 of 2011: {predicted_quantity} Units')\n",
    "\n",
    "# Filter actual data for week 42 of 2011\n",
    "actual_weekly_sales = test_data.groupBy(\"Year\", \"Week\").agg({\"Quantity\": \"sum\"})\n",
    "actual_quantity = actual_weekly_sales.filter(col('Week') == 42).select(\"sum(Quantity)\").collect()[0][0]\n",
    "\n",
    "if actual_quantity is not None:\n",
    "    print(f'Actual Quantity Sold week 42 of 2011: {actual_quantity} Units')\n",
    "    \n",
    "    # Compare predicted vs actual\n",
    "    print(f'Comparison - Predicted vs Actual:')\n",
    "    print(f'Predicted: {predicted_quantity} Units')\n",
    "    print(f'Actual: {actual_quantity} Units')\n",
    "    difference = predicted_quantity - actual_quantity\n",
    "    print(f'Difference: {difference} Units')\n",
    "else:\n",
    "    print('No actual data found for week 42 of 2011 in the test dataset.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Recommendations\n",
    "\n",
    "The model could benefit from more features. \n",
    "\n",
    "While the LinearRegression model is performing better than other regressors based on the MAE comparison, it's still not perfectly accurate. The negative difference suggests that there may be underlying patterns or factors not captured effectively by the current features and model setup.\n",
    "\n",
    "Explore additional relevant features that might influence sales, such as promotional activities, economic indicators, seasonality adjustments, or customer demographics.\n",
    "\n",
    "Model Tuning: Fine-tune the parameters of the LinearRegression model or explore other algorithms that might capture non-linear relationships better.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
